{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80f2a08-3d41-4ac6-af12-db30e64522fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "949a68e1-5064-4402-9ade-f0f7241799ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee104b9-60ac-4c83-acbd-49476dbfa0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [torch.rand(5)[0].item() for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff3b6485-5570-47b7-98f5-7d9a900c74ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got float"
     ]
    }
   ],
   "source": [
    "torch.concat(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c12e1860-ec8b-4b29-876c-ba7ed7b1571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from fastai.layers import Flatten\n",
    "###############################################################################################\n",
    "# Standard resnet\n",
    "\n",
    "def conv(in_planes, out_planes, stride=1, kernel_size=3):\n",
    "    \"convolution with padding\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=(kernel_size-1)//2, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock1d(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, kernel_size=[3,3], downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if(isinstance(kernel_size,int)): kernel_size = [kernel_size,kernel_size//2+1]\n",
    "\n",
    "        self.conv1 = conv(inplanes, planes, stride=stride, kernel_size=kernel_size[0])\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv(planes, planes,kernel_size=kernel_size[1])\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck1d(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, kernel_size=3, downsample=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=kernel_size, stride=stride,\n",
    "                               padding=(kernel_size-1)//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet1d(nn.Sequential):\n",
    "    '''1d adaptation of the torchvision resnet'''\n",
    "    def __init__(self, block, layers, kernel_size=3, num_classes=2, input_channels=3, inplanes=64, fix_feature_dim=True, kernel_size_stem = None, stride_stem=2, pooling_stem=True, stride=2,lin_ftrs_head=None, ps_head=0.5, bn_final_head=False, bn_head=True, act_head=\"relu\", concat_pooling=True):\n",
    "        self.inplanes = inplanes\n",
    "\n",
    "        layers_tmp = []\n",
    "\n",
    "        if(kernel_size_stem is None):\n",
    "            kernel_size_stem = kernel_size[0] if isinstance(kernel_size,list) else kernel_size\n",
    "        #stem\n",
    "        layers_tmp.append(nn.Conv1d(input_channels, inplanes, kernel_size=kernel_size_stem, stride=stride_stem, padding=(kernel_size_stem-1)//2,bias=False))\n",
    "        layers_tmp.append(nn.BatchNorm1d(inplanes))\n",
    "        layers_tmp.append(nn.ReLU(inplace=True))\n",
    "        if(pooling_stem is True):\n",
    "            layers_tmp.append(nn.MaxPool1d(kernel_size=3, stride=2, padding=1))\n",
    "        #backbone\n",
    "        for i,l in enumerate(layers):\n",
    "            if(i==0):\n",
    "                layers_tmp.append(self._make_layer(block, inplanes, layers[0],kernel_size=kernel_size))\n",
    "            else:\n",
    "                layers_tmp.append(self._make_layer(block, inplanes if fix_feature_dim else (2**i)*inplanes, layers[i], stride=stride,kernel_size=kernel_size))\n",
    "        \n",
    "        #head\n",
    "        #layers_tmp.append(nn.AdaptiveAvgPool1d(1))\n",
    "        #layers_tmp.append(Flatten())\n",
    "        #layers_tmp.append(nn.Linear((inplanes if fix_feature_dim else (2**len(layers)*inplanes)) * block.expansion, num_classes))\n",
    "        \n",
    "        # head = create_head1d((inplanes if fix_feature_dim else (2**len(layers)*inplanes)) * block.expansion, nc=num_classes, lin_ftrs=lin_ftrs_head, ps=ps_head, bn_final=bn_final_head, bn=bn_head, act=act_head, concat_pooling=concat_pooling)\n",
    "        # layers_tmp.append(head)\n",
    "        \n",
    "        super().__init__(*layers_tmp)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1,kernel_size=3):\n",
    "        downsample = None\n",
    "        \n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, kernel_size, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def get_layer_groups(self):\n",
    "        return (self[6],self[-1])\n",
    "    \n",
    "    def get_output_layer(self):\n",
    "        return self[-1][-1]\n",
    "        \n",
    "    def set_output_layer(self,x):\n",
    "        self[-1][-1]=x\n",
    "\n",
    "def resnet1d18(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    return ResNet1d(BasicBlock1d, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "def resnet1d34(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    \"\"\"\n",
    "    return ResNet1d(BasicBlock1d, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "def resnet1d50(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    \"\"\"\n",
    "    return ResNet1d(Bottleneck1d, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "def resnet1d101(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    return ResNet1d(Bottleneck1d, [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "def resnet1d152(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    \"\"\"\n",
    "    return ResNet1d(Bottleneck1d, [3, 8, 36, 3], **kwargs)\n",
    "\n",
    "\n",
    "#original used kernel_size_stem = 8\n",
    "def resnet1d_wang(**kwargs):\n",
    "    \n",
    "    if(not(\"kernel_size\" in kwargs.keys())):\n",
    "        kwargs[\"kernel_size\"]=[5,3]\n",
    "    if(not(\"kernel_size_stem\" in kwargs.keys())):\n",
    "        kwargs[\"kernel_size_stem\"]=7\n",
    "    if(not(\"stride_stem\" in kwargs.keys())):\n",
    "        kwargs[\"stride_stem\"]=1\n",
    "    if(not(\"pooling_stem\" in kwargs.keys())):\n",
    "        kwargs[\"pooling_stem\"]=False\n",
    "    if(not(\"inplanes\" in kwargs.keys())):\n",
    "        kwargs[\"inplanes\"]=128\n",
    "\n",
    "\n",
    "    return ResNet1d(BasicBlock1d, [1, 1, 1], **kwargs)\n",
    "\n",
    "def resnet1d(**kwargs):\n",
    "    \"\"\"Constructs a custom ResNet model.\n",
    "    \"\"\"\n",
    "    return ResNet1d(BasicBlock1d, **kwargs)\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "# wide resnet adopted from fastai wrn\n",
    "\n",
    "def noop(x): return x\n",
    "\n",
    "def conv1d(ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None, bias=False) -> nn.Conv1d:\n",
    "    \"Create `nn.Conv1d` layer: `ni` inputs, `nf` outputs, `ks` kernel size. `padding` defaults to `k//2`.\"\n",
    "    if padding is None: padding = ks//2\n",
    "    return nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "def _bn1d(ni, init_zero=False):\n",
    "    \"Batchnorm layer with 0 initialization\"\n",
    "    m = nn.BatchNorm1d(ni)\n",
    "    m.weight.data.fill_(0 if init_zero else 1)\n",
    "    m.bias.data.zero_()\n",
    "    return m\n",
    "\n",
    "def bn_relu_conv1d(ni, nf, ks, stride, init_zero=False):\n",
    "    bn_initzero = _bn1d(ni, init_zero=init_zero)\n",
    "    return nn.Sequential(bn_initzero, nn.ReLU(inplace=True), conv1d(ni, nf, ks, stride))\n",
    "\n",
    "class BasicBlock1dwrn(nn.Module):\n",
    "    def __init__(self, ni, nf, stride, drop_p=0.0, ks=3):\n",
    "        super().__init__()\n",
    "        if(isinstance(ks,int)):\n",
    "            ks = [ks,ks//2+1]\n",
    "        self.bn = nn.BatchNorm1d(ni)\n",
    "        self.conv1 = conv1d(ni, nf, ks[0], stride)\n",
    "        self.conv2 = bn_relu_conv1d(nf, nf, ks[0], 1)\n",
    "        self.drop = nn.Dropout(drop_p, inplace=True) if drop_p else None\n",
    "        self.shortcut = conv1d(ni, nf, ks[1], stride) if (ni != nf or stride>1) else noop #adapted to make it work for fix_feature_dim=True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = F.relu(self.bn(x), inplace=True)\n",
    "        r = self.shortcut(x2)\n",
    "        x = self.conv1(x2)\n",
    "        if self.drop: x = self.drop(x)\n",
    "        x = self.conv2(x) * 0.2\n",
    "        return x.add_(r)\n",
    "\n",
    "def _make_group(N, ni, nf, block, stride, drop_p,ks=3):\n",
    "    return [block(ni if i == 0 else nf, nf, stride if i == 0 else 1, drop_p,ks=ks) for i in range(N)]\n",
    "\n",
    "class WideResNet1d(nn.Sequential):\n",
    "    def __init__(self, input_channels:int, num_groups:int, N:int, k:int=1, drop_p:float=0.0, start_nf:int=16,fix_feature_dim=True,kernel_size=5,lin_ftrs_head=None, ps_head=0.5, bn_final_head=False, bn_head=True, act_head=\"relu\", concat_pooling=True):\n",
    "        super().__init__()\n",
    "        n_channels = [start_nf]\n",
    "        \n",
    "        for i in range(num_groups): n_channels.append(start_nf if fix_feature_dim else start_nf*(2**i)*k)\n",
    "\n",
    "        layers = [conv1d(input_channels, n_channels[0], 3, 1)]  # conv1 stem\n",
    "        for i in range(num_groups):\n",
    "            layers += _make_group(N, n_channels[i], n_channels[i+1], BasicBlock1dwrn, (1 if i==0 else 2), drop_p,ks=kernel_size)\n",
    "\n",
    "        #layers += [nn.BatchNorm1d(n_channels[-1]), nn.ReLU(inplace=True), nn.AdaptiveAvgPool1d(1),\n",
    "        #           Flatten(), nn.Linear(n_channels[-1], num_classes)]\n",
    "        # head = create_head1d(n_channels[-1], nc=num_classes, lin_ftrs=lin_ftrs_head, ps=ps_head, bn_final=bn_final_head, bn=bn_head, act=act_head, concat_pooling=concat_pooling)\n",
    "        # layers.append(head)\n",
    "        \n",
    "        super().__init__(*layers)\n",
    "    \n",
    "    def get_layer_groups(self):\n",
    "        return (self[6],self[-1])\n",
    "    \n",
    "    def get_output_layer(self):\n",
    "        return self[-1][-1]\n",
    "    \n",
    "    def set_output_layer(self,x):\n",
    "        self[-1][-1] = x\n",
    "\n",
    "\n",
    "def wrn1d_22(**kwargs): return WideResNet1d(num_groups=3, N=3, k=6, drop_p=0.,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f27287bf-dad8-45f1-9bb5-ef32ddc5dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3f5ac59-4f5a-4bbf-af03-72013116084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = wrn1d_22(input_channels=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97b31131-0935-483a-9655-0fe71c6fc968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Param #                   Input Shape               Output Shape              Kernel Shape\n",
       "============================================================================================================================================\n",
       "WideResNet1d                             --                        [1, 12, 1000]             [1, 16, 250]              --\n",
       "├─Conv1d: 1-1                            576                       [1, 12, 1000]             [1, 16, 1000]             [3]\n",
       "├─BasicBlock1dwrn: 1-2                   --                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    └─BatchNorm1d: 2-1                  32                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    └─Conv1d: 2-2                       1,280                     [1, 16, 1000]             [1, 16, 1000]             [5]\n",
       "│    └─Sequential: 2-3                   --                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    │    └─BatchNorm1d: 3-1             32                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    │    └─ReLU: 3-2                    --                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    │    └─Conv1d: 3-3                  1,280                     [1, 16, 1000]             [1, 16, 1000]             [5]\n",
       "├─BasicBlock1dwrn: 1-3                   --                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    └─BatchNorm1d: 2-4                  32                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    └─Conv1d: 2-5                       1,280                     [1, 16, 1000]             [1, 16, 1000]             [5]\n",
       "│    └─Sequential: 2-6                   --                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    │    └─BatchNorm1d: 3-4             32                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    │    └─ReLU: 3-5                    --                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    │    └─Conv1d: 3-6                  1,280                     [1, 16, 1000]             [1, 16, 1000]             [5]\n",
       "├─BasicBlock1dwrn: 1-4                   --                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    └─BatchNorm1d: 2-7                  32                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    └─Conv1d: 2-8                       1,280                     [1, 16, 1000]             [1, 16, 1000]             [5]\n",
       "│    └─Sequential: 2-9                   --                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    │    └─BatchNorm1d: 3-7             32                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    │    └─ReLU: 3-8                    --                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    │    └─Conv1d: 3-9                  1,280                     [1, 16, 1000]             [1, 16, 1000]             [5]\n",
       "├─BasicBlock1dwrn: 1-5                   --                        [1, 16, 1000]             [1, 16, 500]              --\n",
       "│    └─BatchNorm1d: 2-10                 32                        [1, 16, 1000]             [1, 16, 1000]             --\n",
       "│    └─Conv1d: 2-11                      768                       [1, 16, 1000]             [1, 16, 500]              [3]\n",
       "│    └─Conv1d: 2-12                      1,280                     [1, 16, 1000]             [1, 16, 500]              [5]\n",
       "│    └─Sequential: 2-13                  --                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    │    └─BatchNorm1d: 3-10            32                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    │    └─ReLU: 3-11                   --                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    │    └─Conv1d: 3-12                 1,280                     [1, 16, 500]              [1, 16, 500]              [5]\n",
       "├─BasicBlock1dwrn: 1-6                   --                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    └─BatchNorm1d: 2-14                 32                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    └─Conv1d: 2-15                      1,280                     [1, 16, 500]              [1, 16, 500]              [5]\n",
       "│    └─Sequential: 2-16                  --                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    │    └─BatchNorm1d: 3-13            32                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    │    └─ReLU: 3-14                   --                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    │    └─Conv1d: 3-15                 1,280                     [1, 16, 500]              [1, 16, 500]              [5]\n",
       "├─BasicBlock1dwrn: 1-7                   --                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    └─BatchNorm1d: 2-17                 32                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    └─Conv1d: 2-18                      1,280                     [1, 16, 500]              [1, 16, 500]              [5]\n",
       "│    └─Sequential: 2-19                  --                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    │    └─BatchNorm1d: 3-16            32                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    │    └─ReLU: 3-17                   --                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    │    └─Conv1d: 3-18                 1,280                     [1, 16, 500]              [1, 16, 500]              [5]\n",
       "├─BasicBlock1dwrn: 1-8                   --                        [1, 16, 500]              [1, 16, 250]              --\n",
       "│    └─BatchNorm1d: 2-20                 32                        [1, 16, 500]              [1, 16, 500]              --\n",
       "│    └─Conv1d: 2-21                      768                       [1, 16, 500]              [1, 16, 250]              [3]\n",
       "│    └─Conv1d: 2-22                      1,280                     [1, 16, 500]              [1, 16, 250]              [5]\n",
       "│    └─Sequential: 2-23                  --                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    │    └─BatchNorm1d: 3-19            32                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    │    └─ReLU: 3-20                   --                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    │    └─Conv1d: 3-21                 1,280                     [1, 16, 250]              [1, 16, 250]              [5]\n",
       "├─BasicBlock1dwrn: 1-9                   --                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    └─BatchNorm1d: 2-24                 32                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    └─Conv1d: 2-25                      1,280                     [1, 16, 250]              [1, 16, 250]              [5]\n",
       "│    └─Sequential: 2-26                  --                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    │    └─BatchNorm1d: 3-22            32                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    │    └─ReLU: 3-23                   --                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    │    └─Conv1d: 3-24                 1,280                     [1, 16, 250]              [1, 16, 250]              [5]\n",
       "├─BasicBlock1dwrn: 1-10                  --                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    └─BatchNorm1d: 2-27                 32                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    └─Conv1d: 2-28                      1,280                     [1, 16, 250]              [1, 16, 250]              [5]\n",
       "│    └─Sequential: 2-29                  --                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    │    └─BatchNorm1d: 3-25            32                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    │    └─ReLU: 3-26                   --                        [1, 16, 250]              [1, 16, 250]              --\n",
       "│    │    └─Conv1d: 3-27                 1,280                     [1, 16, 250]              [1, 16, 250]              [5]\n",
       "============================================================================================================================================\n",
       "Total params: 25,728\n",
       "Trainable params: 25,728\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 14.59\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 3.01\n",
       "Params size (MB): 0.10\n",
       "Estimated Total Size (MB): 3.16\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net, depth=5, col_names=['num_params', 'input_size', 'output_size', 'kernel_size'], input_size=(1, 12, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c38220-e3dc-42bb-a18c-a693851443c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
