{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20171a17-a1b6-4edb-802d-f413a2768e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "from funcs import run_all_ml_experiments, run_ml_experiment, run_dl_experiment\n",
    "from msr.training.data.transforms import Flatten, Permute\n",
    "\n",
    "from msr.models.modules import ClassifierModule\n",
    "from msr.models.architectures.networks.mlp import MLPClassifier\n",
    "from msr.models.architectures.networks.cnn import CNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0578aa-4c64-4b23-8326-2e77df1b7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_transform = Flatten(start_dim=0, end_dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f0961f3-d567-48e7-90ac-01b9b8e1aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_model_provider():\n",
    "    return LGBMClassifier(n_jobs=-1)\n",
    "\n",
    "def rf_model_provider():\n",
    "    return RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "def lr_model_provider():\n",
    "    return LogisticRegression(n_jobs=-1)\n",
    "\n",
    "def mlp_model_provider(num_classes, input_shape):\n",
    "    input_size = input_shape[0]\n",
    "    net = MLPClassifier(\n",
    "        num_classes=num_classes,\n",
    "        input_size=input_size,\n",
    "        hidden_dims=[input_size // (2**i) for i in range(1, 4)]\n",
    "    )\n",
    "    return ClassifierModule(net=net)\n",
    "\n",
    "def cnn_model_provider(num_classes, input_shape):\n",
    "    net = CNNClassifier(\n",
    "        num_classes=num_classes, \n",
    "        dim=1,\n",
    "        in_channels=12,\n",
    "        out_channels=[4],\n",
    "        kernel_size=[11],\n",
    "        maxpool_kernel_size=[2]\n",
    "    )\n",
    "    return ClassifierModule(net=net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe3d71-cc50-4c86-8942-743141993adb",
   "metadata": {},
   "source": [
    "# **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0bb632b6-7640-4ca5-b120-a2db1b9cde1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42147706cb014e53b9688d7c86132c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole_signal_waveforms 0.7158476710319519\n",
      "whole_signal_features 0.8975158929824829\n",
      "agg_beat_waveforms 0.8873464465141296\n",
      "agg_beat_features 0.9018481969833374\n"
     ]
    }
   ],
   "source": [
    "rf_results = run_all_ml_experiments(rf_model_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee842d3b-d243-488a-8175-9bcc682f1fee",
   "metadata": {},
   "source": [
    "# **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3fa7e15-abec-4567-a3b3-43007bd3100a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3dc76f3d5a47549dc88a851c988a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole_signal_waveforms 0.49\n",
      "whole_signal_features 0.84\n",
      "agg_beat_waveforms 0.85\n",
      "agg_beat_features 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shate/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/shate/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/shate/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/shate/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr_results = run_all_ml_experiments(lr_model_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2d0a9-a663-407a-8124-db86aa8af5b7",
   "metadata": {},
   "source": [
    "# **MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a3024ab-77b4-4f7d-bf2a-e593987c14ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shate/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | net       | MLPClassifier | 577 K \n",
      "1 | criterion | NLLLoss       | 0     \n",
      "--------------------------------------------\n",
      "577 K     Trainable params\n",
      "0         Non-trainable params\n",
      "577 K     Total params\n",
      "2.312     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0206, -0.0125,  0.0161,  ...,  0.0168,  0.0063,  0.0295],\n",
      "        [-0.0008,  0.0275, -0.0026,  ..., -0.0172, -0.0181, -0.0264],\n",
      "        [-0.0216,  0.0032,  0.0164,  ...,  0.0061, -0.0017, -0.0004],\n",
      "        ...,\n",
      "        [ 0.0039, -0.0243, -0.0296,  ...,  0.0103,  0.0152, -0.0262],\n",
      "        [ 0.0156,  0.0094,  0.0197,  ...,  0.0080, -0.0242, -0.0310],\n",
      "        [-0.0165,  0.0033, -0.0165,  ...,  0.0215,  0.0280, -0.0278]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 1.9780e-02, -3.4932e-03, -1.1202e-02,  1.0865e-02,  5.7630e-03,\n",
      "         2.8192e-02,  1.8542e-02,  1.6307e-03,  1.8252e-02, -2.1169e-02,\n",
      "         1.2541e-02,  2.5700e-02,  3.2680e-02, -2.6633e-02, -6.4885e-03,\n",
      "         1.1153e-02,  1.7467e-02,  3.1970e-02, -2.8249e-02,  2.7227e-02,\n",
      "        -2.7243e-03, -1.5456e-02, -3.3785e-03,  7.5175e-03, -2.0010e-02,\n",
      "         2.6150e-02, -1.0684e-03,  1.7054e-03,  2.5239e-02,  2.4588e-02,\n",
      "        -8.0670e-03, -2.6568e-02,  1.4314e-02, -3.1265e-02,  3.2438e-02,\n",
      "         3.1743e-02, -2.6158e-02, -1.5604e-02,  5.1071e-03, -4.9345e-03,\n",
      "        -2.2131e-02,  3.0487e-02, -5.4474e-03, -2.8961e-03,  1.8241e-02,\n",
      "         2.8956e-02, -1.3823e-02,  2.9108e-02, -1.2877e-02,  2.0442e-02,\n",
      "        -1.6093e-03,  2.4927e-02,  1.9916e-02, -8.9064e-03,  1.3150e-02,\n",
      "         1.3517e-03, -2.1003e-02,  4.5075e-03, -2.8912e-02, -2.5006e-02,\n",
      "         1.6628e-02, -1.5339e-02,  2.2938e-02,  2.0240e-02, -3.3883e-03,\n",
      "         2.5803e-03, -3.2059e-02,  3.2162e-02, -1.1046e-02,  2.0570e-02,\n",
      "        -2.2264e-02, -2.0252e-02, -1.2858e-02, -3.1687e-02, -6.0961e-03,\n",
      "        -5.9340e-03, -3.8398e-03,  4.1045e-03,  9.9088e-03,  1.5008e-02,\n",
      "         1.9276e-03, -2.5409e-02,  3.1556e-02,  2.7568e-02,  1.9671e-02,\n",
      "         2.0143e-02, -1.4272e-03, -4.3428e-03,  1.9608e-02,  2.0693e-02,\n",
      "        -4.7970e-03, -5.5233e-04, -1.5897e-04, -2.7193e-02,  2.3078e-02,\n",
      "         2.2943e-02,  2.3218e-02, -1.1489e-02, -8.3509e-03,  1.1812e-02,\n",
      "         2.3353e-02,  3.7231e-03,  2.6881e-02, -1.6330e-03, -5.2430e-03,\n",
      "         5.7434e-03, -1.5798e-02,  1.2457e-02,  2.0704e-02,  3.2429e-02,\n",
      "         1.1242e-02,  2.9609e-02, -1.5322e-02,  3.0203e-02, -2.4906e-02,\n",
      "         1.4759e-02, -2.3096e-02,  2.5250e-02, -4.8072e-03, -9.3584e-03,\n",
      "         1.1168e-02, -9.8836e-03,  3.1221e-02,  2.4331e-02,  1.7044e-02,\n",
      "        -2.6126e-02, -1.8459e-02, -1.8939e-03, -2.3156e-02, -1.8149e-02,\n",
      "         2.8813e-02,  2.3794e-02, -1.7670e-03,  1.9105e-02,  2.1372e-02,\n",
      "        -1.7591e-02,  2.3137e-02,  3.0900e-02,  2.0535e-02, -1.7352e-02,\n",
      "        -9.6681e-03, -4.2585e-03,  1.3188e-02, -8.6485e-03,  1.7543e-02,\n",
      "         1.0967e-02, -2.2558e-02,  2.1483e-02, -2.5186e-02,  2.2461e-02,\n",
      "         1.1664e-02, -1.3866e-04, -2.9085e-02, -1.1062e-02,  3.1608e-02,\n",
      "        -1.3393e-02, -4.5084e-03, -2.9105e-02, -1.4412e-04,  2.0727e-02,\n",
      "         2.0941e-02,  1.7741e-02, -8.9786e-03,  1.7992e-02, -2.0124e-02,\n",
      "         2.2523e-02, -1.5535e-02,  3.1795e-02, -1.9535e-02,  2.2166e-02,\n",
      "        -2.8480e-02,  3.2997e-03, -2.7883e-02, -1.1185e-03,  2.2542e-02,\n",
      "         1.7949e-02,  2.5095e-02,  4.9339e-03,  7.8615e-03, -6.4103e-04,\n",
      "        -1.4774e-02,  3.1820e-02, -2.7502e-02, -2.7633e-02, -2.3231e-02,\n",
      "        -6.8021e-03,  1.9077e-02, -2.6474e-02, -2.0720e-02, -2.3124e-03,\n",
      "        -1.6940e-02, -3.2006e-02, -2.2488e-02,  1.5796e-02, -2.9976e-02,\n",
      "        -3.2059e-02, -2.8001e-02,  1.6284e-02,  1.1216e-02,  3.1829e-02,\n",
      "        -2.1754e-02, -1.9131e-03, -2.4379e-02, -2.4403e-02, -1.1222e-02,\n",
      "        -1.0182e-02, -1.4189e-02,  4.4697e-03,  7.3430e-03, -3.1973e-02,\n",
      "        -1.4646e-02, -2.1290e-02, -2.8813e-02, -2.0064e-04,  1.3984e-02,\n",
      "         1.7879e-03, -1.8193e-02, -2.6107e-02,  5.7650e-03, -1.1996e-02,\n",
      "        -3.1712e-02,  7.0997e-04, -2.8911e-02, -1.6782e-02, -1.4510e-02,\n",
      "         3.2390e-02,  1.8365e-03,  1.7738e-02,  1.7345e-03, -1.0972e-02,\n",
      "         6.6456e-03, -1.8664e-02, -4.8006e-03,  1.6473e-02,  2.6400e-02,\n",
      "        -1.9052e-02,  2.5649e-03,  3.2322e-02,  2.9614e-02, -2.4172e-03,\n",
      "         2.5452e-02, -5.5649e-03,  1.0711e-02, -7.8447e-03, -3.1596e-02,\n",
      "        -2.6833e-02, -1.6369e-02, -1.3711e-02,  2.9968e-02, -8.8731e-03,\n",
      "         3.1877e-02,  1.9684e-02, -2.9499e-02, -2.0225e-02,  1.5298e-02,\n",
      "        -7.3387e-03,  3.0337e-02, -2.9522e-02, -1.0000e-02, -7.1497e-03,\n",
      "         1.1610e-02,  2.3804e-02, -3.0510e-03,  1.7326e-02,  1.1214e-02,\n",
      "        -1.8863e-02, -5.9234e-03, -1.6164e-02, -6.4553e-03, -3.0719e-02,\n",
      "         3.0987e-02,  2.8318e-02, -8.7519e-03,  1.9380e-02,  3.1483e-02,\n",
      "         2.2060e-02, -1.7625e-02, -2.1230e-02, -1.9886e-02,  2.8492e-03,\n",
      "        -1.4245e-02,  2.4057e-02, -3.2161e-03, -2.2375e-02, -2.7397e-02,\n",
      "        -7.4064e-03,  2.6716e-02, -4.2713e-03, -2.6914e-02,  3.2162e-02,\n",
      "         8.7304e-03, -6.9771e-03, -1.1051e-02, -1.8527e-03, -1.2913e-02,\n",
      "        -2.8070e-02,  1.8072e-02, -2.5790e-02, -3.2467e-03,  5.7456e-03,\n",
      "        -7.4774e-03, -2.2155e-03,  6.1793e-03, -7.4016e-03,  3.1072e-02,\n",
      "         2.5404e-02, -1.8543e-02, -1.8825e-02, -8.4028e-03,  1.7823e-02,\n",
      "        -2.2653e-02, -5.0844e-03, -1.3115e-02,  1.4282e-02,  1.8452e-02,\n",
      "        -1.5637e-02,  3.1861e-02,  2.4060e-02,  8.6434e-03,  2.9128e-02,\n",
      "        -8.0898e-03, -1.6970e-02, -8.0868e-03,  1.3247e-02, -1.6117e-02,\n",
      "        -8.0859e-03,  3.2418e-02, -1.5442e-03,  2.1961e-02, -1.2145e-02,\n",
      "         2.2620e-02, -1.8632e-02, -1.5378e-03, -1.9103e-02, -3.2328e-03,\n",
      "         2.2004e-02,  9.3152e-03, -1.8369e-02, -7.9246e-03, -3.2295e-03,\n",
      "         1.8793e-03, -1.5807e-02, -1.2328e-02, -1.7692e-02, -2.3887e-02,\n",
      "        -2.3034e-02,  2.3812e-02,  5.4738e-03, -3.0882e-02, -3.1812e-02,\n",
      "         3.1723e-02, -3.0158e-02, -1.0269e-02,  1.5341e-02,  2.8546e-02,\n",
      "        -2.2009e-02, -4.2873e-04, -2.6722e-02, -7.4830e-03, -1.1329e-02,\n",
      "         2.4642e-02, -2.5561e-03,  2.7563e-02,  3.1672e-02,  8.2981e-03,\n",
      "         3.1388e-02,  1.9365e-03,  7.8895e-03, -1.2835e-02,  1.6866e-02,\n",
      "         2.4924e-02, -2.9070e-02, -2.1997e-02,  1.1336e-02, -2.5965e-02,\n",
      "         2.8065e-02, -1.8444e-02,  2.0096e-02,  2.2145e-03,  7.8196e-03,\n",
      "        -8.0948e-03, -1.0968e-02, -2.1100e-02, -1.2300e-02,  1.5769e-02,\n",
      "         2.8624e-02, -2.5315e-02, -2.7798e-02,  1.2676e-02, -2.9604e-02,\n",
      "         1.3663e-02, -1.7959e-02,  2.5146e-02,  2.1454e-03, -1.5373e-03,\n",
      "        -1.2555e-02,  6.0108e-03, -3.0866e-02,  3.0124e-02, -2.4265e-02,\n",
      "        -2.0307e-02, -2.0740e-02,  1.2586e-02, -1.2207e-03,  2.0807e-02,\n",
      "        -8.4329e-03,  3.5092e-04, -1.0989e-02, -2.1167e-02,  6.2129e-04,\n",
      "         2.1205e-02, -2.3696e-02, -5.4381e-03, -2.1765e-02,  9.0792e-03,\n",
      "        -4.7643e-04, -5.9213e-03,  1.2465e-02, -1.3477e-02,  1.4780e-02,\n",
      "         2.2000e-02, -8.8762e-03, -2.7882e-02, -3.2054e-03, -2.3871e-02,\n",
      "        -3.1584e-02,  2.4132e-02, -1.4970e-02,  3.0003e-02, -3.0735e-02,\n",
      "        -3.8375e-03,  4.1134e-04,  8.9558e-03,  3.0689e-02,  3.9553e-03,\n",
      "        -3.1648e-02, -1.2037e-03, -1.3463e-03,  2.7532e-02, -1.3962e-02,\n",
      "        -1.2434e-02,  3.0477e-02, -1.4136e-02,  4.6257e-03, -2.2545e-02,\n",
      "        -1.9749e-03, -2.2554e-03, -2.5738e-02,  1.9382e-02, -2.3436e-02,\n",
      "         1.5983e-05, -2.6266e-02,  1.4526e-02,  1.1744e-02, -1.5052e-02,\n",
      "         3.4943e-03,  9.5154e-03, -1.5207e-02,  8.4943e-03, -1.6682e-02,\n",
      "        -3.0722e-02,  2.6624e-02, -3.1720e-02,  4.9873e-03, -1.1286e-02,\n",
      "        -1.4534e-02, -2.8678e-02, -1.1871e-02], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0394,  0.0081, -0.0147,  ...,  0.0241, -0.0320,  0.0058],\n",
      "        [-0.0180, -0.0327, -0.0410,  ...,  0.0400,  0.0462,  0.0179],\n",
      "        [ 0.0204,  0.0040,  0.0126,  ..., -0.0283,  0.0112, -0.0184],\n",
      "        ...,\n",
      "        [-0.0287,  0.0177,  0.0187,  ..., -0.0123, -0.0121,  0.0182],\n",
      "        [-0.0410,  0.0007, -0.0097,  ...,  0.0121,  0.0187, -0.0308],\n",
      "        [ 0.0369, -0.0320, -0.0095,  ..., -0.0240, -0.0327,  0.0377]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 4.5264e-02, -6.6210e-03, -1.3303e-02, -3.8529e-02,  3.7272e-02,\n",
      "        -3.0427e-02, -4.5678e-02, -1.6512e-02, -1.2674e-02,  1.6197e-02,\n",
      "         8.2326e-03,  2.8872e-02,  3.5526e-02, -2.1606e-02, -3.9813e-03,\n",
      "        -4.0238e-02,  6.6120e-03,  1.4743e-02, -1.0848e-02,  4.3716e-02,\n",
      "        -4.4474e-02, -3.8045e-02, -8.8443e-03, -1.1441e-02, -4.3770e-02,\n",
      "        -1.3910e-02,  1.1923e-02,  4.4973e-02,  6.7640e-03,  3.5820e-02,\n",
      "        -3.3243e-02,  3.2641e-02,  3.0986e-02,  3.7312e-02,  3.1862e-02,\n",
      "        -1.8287e-02, -2.4378e-03,  2.2965e-02,  2.7476e-02, -1.3041e-02,\n",
      "        -2.9573e-02, -1.1198e-02, -7.7496e-03,  1.6236e-02, -3.3253e-02,\n",
      "         2.6722e-02,  3.6667e-02, -4.2585e-02,  1.2197e-02,  7.3122e-03,\n",
      "        -4.1953e-02,  6.0266e-03, -2.7374e-02, -1.8713e-02,  1.0671e-02,\n",
      "        -2.5306e-02, -1.4407e-02,  3.2392e-02,  1.6887e-02,  2.5159e-02,\n",
      "        -3.8511e-02, -1.2297e-02,  4.3574e-02,  3.4643e-02, -2.8923e-02,\n",
      "         3.6859e-02, -2.0257e-02,  4.0790e-02,  2.2714e-02, -4.1128e-02,\n",
      "        -9.3728e-03, -2.9441e-03, -3.9299e-02,  1.2783e-02, -2.3919e-02,\n",
      "         4.3943e-02,  3.8362e-02,  2.8750e-02,  4.7762e-03,  3.7450e-02,\n",
      "        -4.0706e-02,  4.5987e-02, -2.9431e-02,  1.3116e-02, -3.9889e-02,\n",
      "         1.5706e-02,  3.8021e-02, -2.2342e-03, -3.2151e-02, -3.6460e-02,\n",
      "        -4.1955e-02, -4.1251e-02, -3.2116e-02, -3.2168e-02,  1.3738e-02,\n",
      "        -2.0686e-02, -3.7511e-03,  3.3223e-02, -1.4482e-02,  7.6110e-03,\n",
      "         1.1068e-02,  3.4259e-02, -4.0991e-02,  4.5321e-02, -6.7749e-03,\n",
      "         3.0677e-02, -4.4478e-02,  8.1449e-03, -1.7021e-02, -1.3370e-02,\n",
      "        -2.1751e-02,  3.4841e-02,  2.9192e-02, -2.9917e-02,  2.0039e-02,\n",
      "         4.3590e-02,  4.6163e-02,  3.6303e-02,  2.4524e-02,  3.0866e-02,\n",
      "        -1.2840e-03, -2.7752e-02, -3.3923e-02, -3.3340e-02,  1.3925e-02,\n",
      "        -3.7913e-02,  4.5036e-02,  9.2880e-03, -2.5640e-02, -1.8244e-02,\n",
      "         3.7030e-02, -1.0293e-03,  4.3808e-03,  1.7094e-02,  4.2988e-02,\n",
      "         3.1832e-02,  1.5415e-02,  3.0002e-02, -4.6140e-02,  2.4368e-03,\n",
      "         3.9095e-02,  1.0272e-02, -6.1091e-03, -9.5440e-03, -3.3238e-02,\n",
      "         1.0993e-05, -4.2854e-02, -9.9844e-03, -2.9843e-02,  3.4224e-02,\n",
      "         3.4737e-02,  2.5153e-02,  3.2079e-02, -1.1643e-02,  2.4355e-02,\n",
      "         4.5657e-02,  3.0217e-02, -1.4560e-03, -8.6479e-03,  1.5698e-03,\n",
      "         1.8709e-02,  1.1693e-02, -4.4347e-02,  1.4824e-02,  1.7344e-02,\n",
      "        -1.7346e-02, -4.3977e-02, -4.3496e-02,  8.7845e-04, -4.3830e-02,\n",
      "         3.6464e-02,  3.4830e-03,  5.6065e-03, -2.3876e-02, -3.3151e-02,\n",
      "         3.4211e-03,  3.4435e-02, -2.6235e-02,  4.0387e-02,  1.2874e-02,\n",
      "         7.2552e-03,  4.3528e-02,  1.8545e-02,  1.5702e-02,  3.9336e-03,\n",
      "        -1.8530e-02,  3.8368e-02, -1.8063e-02, -1.2216e-02, -2.8795e-02,\n",
      "         7.1236e-03,  2.4357e-02,  1.7188e-02, -1.5516e-02,  3.9699e-02,\n",
      "        -2.1699e-02,  2.0328e-03, -2.8031e-02, -3.6658e-02,  2.4225e-02,\n",
      "        -1.0227e-02, -3.9698e-02,  1.1819e-02, -3.8013e-02,  4.4927e-02,\n",
      "         2.0786e-02,  4.7620e-03,  9.2871e-03, -3.1465e-02,  4.6245e-03,\n",
      "         2.2056e-02,  3.0158e-02,  9.6065e-03,  4.3307e-02,  1.1612e-02,\n",
      "         2.9605e-02,  3.8911e-02,  2.8377e-02, -3.3175e-02,  3.5050e-02,\n",
      "         1.6060e-02, -4.7182e-03, -3.6140e-02,  7.2000e-03,  2.1228e-02,\n",
      "         4.0972e-03,  3.5909e-02,  2.1762e-02, -4.2702e-03, -1.4005e-02,\n",
      "        -4.5757e-02, -2.6738e-02, -2.3768e-02, -1.6453e-02], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0466,  0.0609,  0.0085,  ...,  0.0252,  0.0566,  0.0273],\n",
      "        [ 0.0177,  0.0054,  0.0324,  ...,  0.0325, -0.0560, -0.0431],\n",
      "        [ 0.0137, -0.0130, -0.0636,  ...,  0.0203, -0.0428, -0.0051],\n",
      "        ...,\n",
      "        [ 0.0547,  0.0028,  0.0004,  ..., -0.0317, -0.0535, -0.0278],\n",
      "        [ 0.0006, -0.0141,  0.0074,  ..., -0.0629,  0.0007, -0.0631],\n",
      "        [ 0.0382, -0.0162,  0.0425,  ..., -0.0469, -0.0260,  0.0485]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0453,  0.0038, -0.0080, -0.0101, -0.0525, -0.0578,  0.0455, -0.0463,\n",
      "         0.0600,  0.0344,  0.0574,  0.0229,  0.0313, -0.0562, -0.0037, -0.0529,\n",
      "         0.0592, -0.0436,  0.0339,  0.0573, -0.0127, -0.0222,  0.0548, -0.0049,\n",
      "         0.0121,  0.0325,  0.0014,  0.0432,  0.0381, -0.0549,  0.0532,  0.0445,\n",
      "         0.0085,  0.0402, -0.0197, -0.0458, -0.0112,  0.0556,  0.0154,  0.0596,\n",
      "         0.0541,  0.0373, -0.0071,  0.0068,  0.0039,  0.0466,  0.0099,  0.0024,\n",
      "         0.0097, -0.0385, -0.0380,  0.0008, -0.0222,  0.0413, -0.0439, -0.0172,\n",
      "         0.0433, -0.0052, -0.0338, -0.0340, -0.0088,  0.0274, -0.0171, -0.0197,\n",
      "        -0.0628,  0.0101, -0.0354,  0.0086,  0.0156, -0.0390,  0.0242, -0.0070,\n",
      "         0.0557, -0.0441,  0.0003,  0.0417, -0.0069,  0.0265,  0.0073,  0.0641,\n",
      "        -0.0437, -0.0262,  0.0583,  0.0118, -0.0093,  0.0108,  0.0404,  0.0343,\n",
      "        -0.0457, -0.0628, -0.0624,  0.0072, -0.0227, -0.0109,  0.0484, -0.0033,\n",
      "         0.0271, -0.0005,  0.0111, -0.0146,  0.0143, -0.0512, -0.0543,  0.0284,\n",
      "        -0.0102, -0.0293, -0.0482,  0.0653, -0.0100, -0.0174,  0.0269,  0.0206,\n",
      "        -0.0012,  0.0214, -0.0474,  0.0473, -0.0258], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0097, -0.0379, -0.0623, -0.0773, -0.0922,  0.0659,  0.0428, -0.0334,\n",
      "          0.0494, -0.0609,  0.0167, -0.0422, -0.0615,  0.0133, -0.0397, -0.0379,\n",
      "         -0.0495,  0.0566, -0.0301,  0.0788,  0.0892, -0.0147, -0.0044,  0.0193,\n",
      "          0.0427, -0.0452, -0.0729, -0.0602,  0.0611,  0.0476,  0.0810,  0.0258,\n",
      "         -0.0302,  0.0567, -0.0011,  0.0526, -0.0621,  0.0405,  0.0133, -0.0146,\n",
      "          0.0187, -0.0870,  0.0595,  0.0382, -0.0171, -0.0274,  0.0113, -0.0341,\n",
      "         -0.0381, -0.0478, -0.0745, -0.0548, -0.0684, -0.0386, -0.0853, -0.0626,\n",
      "          0.0831,  0.0624,  0.0338,  0.0678,  0.0206,  0.0150,  0.0417, -0.0806,\n",
      "         -0.0711, -0.0756, -0.0125,  0.0137,  0.0114, -0.0319,  0.0825,  0.0791,\n",
      "         -0.0602, -0.0665, -0.0722,  0.0385,  0.0095,  0.0784, -0.0387,  0.0126,\n",
      "         -0.0130,  0.0436,  0.0140, -0.0516, -0.0767, -0.0802, -0.0417, -0.0677,\n",
      "         -0.0268,  0.0588,  0.0037, -0.0263,  0.0329, -0.0062, -0.0502, -0.0536,\n",
      "          0.0649, -0.0022, -0.0279, -0.0713, -0.0794,  0.0922,  0.0521, -0.0377,\n",
      "          0.0665, -0.0859,  0.0707,  0.0834,  0.0673, -0.0906, -0.0769, -0.0183,\n",
      "          0.0693,  0.0749, -0.0575, -0.0700, -0.0790],\n",
      "        [ 0.0643,  0.0690,  0.0687,  0.0724, -0.0392,  0.0891, -0.0072,  0.0362,\n",
      "         -0.0574, -0.0330, -0.0784, -0.0846,  0.0243,  0.0031,  0.0646,  0.0240,\n",
      "          0.0059,  0.0657,  0.0230, -0.0695, -0.0765,  0.0228, -0.0058, -0.0882,\n",
      "         -0.0222, -0.0771, -0.0481,  0.0019,  0.0057, -0.0447,  0.0307, -0.0225,\n",
      "         -0.0213, -0.0148,  0.0452,  0.0079, -0.0142,  0.0720, -0.0350, -0.0083,\n",
      "          0.0579,  0.0360,  0.0009,  0.0108,  0.0642, -0.0645,  0.0547,  0.0668,\n",
      "         -0.0807, -0.0291,  0.0142,  0.0282,  0.0166, -0.0273,  0.0906, -0.0342,\n",
      "         -0.0360, -0.0563,  0.0824, -0.0252, -0.0120,  0.0562, -0.0647,  0.0183,\n",
      "          0.0173,  0.0360,  0.0675,  0.0636,  0.0666, -0.0276,  0.0141, -0.0590,\n",
      "          0.0479, -0.0107,  0.0095, -0.0918,  0.0244,  0.0798, -0.0758, -0.0060,\n",
      "          0.0734, -0.0816,  0.0663, -0.0670, -0.0017, -0.0219,  0.0413, -0.0561,\n",
      "         -0.0707, -0.0614,  0.0292,  0.0492,  0.0879,  0.0812, -0.0342, -0.0818,\n",
      "         -0.0617,  0.0243,  0.0417,  0.0462,  0.0016, -0.0681,  0.0399,  0.0627,\n",
      "          0.0125,  0.0366,  0.0383, -0.0166, -0.0783,  0.0412,  0.0464, -0.0346,\n",
      "         -0.0230, -0.0897, -0.0802, -0.0476,  0.0280],\n",
      "        [-0.0205, -0.0318, -0.0161, -0.0139, -0.0793, -0.0202,  0.0768, -0.0535,\n",
      "          0.0772,  0.0919, -0.0371, -0.0324,  0.0904, -0.0061, -0.0389, -0.0489,\n",
      "          0.0118, -0.0420, -0.0272, -0.0574, -0.0285,  0.0558,  0.0361, -0.0759,\n",
      "         -0.0180,  0.0197,  0.0352, -0.0601,  0.0060, -0.0196, -0.0172, -0.0537,\n",
      "         -0.0800,  0.0626,  0.0483, -0.0661,  0.0308,  0.0336, -0.0385,  0.0829,\n",
      "          0.0003, -0.0814,  0.0896, -0.0251,  0.0330, -0.0015,  0.0215,  0.0842,\n",
      "          0.0630, -0.0385, -0.0088,  0.0782,  0.0794,  0.0262,  0.0412,  0.0121,\n",
      "         -0.0526, -0.0356,  0.0199,  0.0190,  0.0861, -0.0146,  0.0133, -0.0058,\n",
      "         -0.0739,  0.0149,  0.0363,  0.0443, -0.0586,  0.0700,  0.0019,  0.0916,\n",
      "         -0.0523,  0.0565, -0.0889, -0.0387, -0.0722,  0.0609, -0.0183, -0.0528,\n",
      "          0.0455,  0.0506, -0.0200, -0.0314, -0.0874, -0.0398,  0.0064,  0.0029,\n",
      "         -0.0324,  0.0817,  0.0302,  0.0799, -0.0918,  0.0426, -0.0145, -0.0104,\n",
      "          0.0289, -0.0131,  0.0128, -0.0364, -0.0093, -0.0811,  0.0317, -0.0770,\n",
      "         -0.0303, -0.0071, -0.0904,  0.0889,  0.0258,  0.0125, -0.0093,  0.0609,\n",
      "          0.0340,  0.0526,  0.0821,  0.0652, -0.0616],\n",
      "        [-0.0680, -0.0730,  0.0814, -0.0865,  0.0407,  0.0266, -0.0604,  0.0498,\n",
      "          0.0237, -0.0165,  0.0281,  0.0166, -0.0226, -0.0798, -0.0093,  0.0031,\n",
      "          0.0566,  0.0704, -0.0691, -0.0161,  0.0864,  0.0016,  0.0885,  0.0769,\n",
      "         -0.0296, -0.0567, -0.0169, -0.0019, -0.0275, -0.0822,  0.0056, -0.0710,\n",
      "         -0.0607,  0.0776,  0.0681,  0.0291,  0.0004, -0.0391,  0.0306,  0.0630,\n",
      "         -0.0396, -0.0510,  0.0399,  0.0116, -0.0252,  0.0903,  0.0626, -0.0275,\n",
      "         -0.0782, -0.0302, -0.0568,  0.0731, -0.0556, -0.0352, -0.0623,  0.0020,\n",
      "         -0.0528,  0.0167, -0.0344, -0.0711, -0.0350, -0.0390, -0.0249, -0.0861,\n",
      "         -0.0131,  0.0880,  0.0251,  0.0391, -0.0341, -0.0713, -0.0191, -0.0623,\n",
      "         -0.0161,  0.0539, -0.0353,  0.0746, -0.0239, -0.0282,  0.0760, -0.0608,\n",
      "          0.0327, -0.0873, -0.0710, -0.0487,  0.0195, -0.0556,  0.0769, -0.0588,\n",
      "         -0.0786, -0.0577, -0.0022,  0.0017, -0.0615,  0.0532, -0.0034,  0.0443,\n",
      "          0.0079,  0.0581, -0.0610, -0.0592, -0.0858, -0.0394,  0.0658,  0.0916,\n",
      "         -0.0362,  0.0348, -0.0168, -0.0801,  0.0341, -0.0632, -0.0317,  0.0326,\n",
      "         -0.0201, -0.0870,  0.0431,  0.0170, -0.0142],\n",
      "        [-0.0488, -0.0694,  0.0533, -0.0481, -0.0638,  0.0229,  0.0511, -0.0678,\n",
      "          0.0236,  0.0804, -0.0838, -0.0886, -0.0173,  0.0527, -0.0470, -0.0575,\n",
      "          0.0330, -0.0172,  0.0250, -0.0825,  0.0124, -0.0514, -0.0329,  0.0244,\n",
      "         -0.0171,  0.0640, -0.0917, -0.0823, -0.0443, -0.0089,  0.0637,  0.0042,\n",
      "          0.0118, -0.0609,  0.0370, -0.0032, -0.0870,  0.0530,  0.0146,  0.0395,\n",
      "         -0.0558,  0.0924, -0.0339, -0.0548, -0.0573,  0.0237, -0.0203, -0.0376,\n",
      "          0.0716, -0.0676, -0.0356, -0.0326,  0.0419, -0.0439,  0.0441,  0.0718,\n",
      "         -0.0132,  0.0777,  0.0359, -0.0472, -0.0338,  0.0417, -0.0805, -0.0688,\n",
      "          0.0422,  0.0666, -0.0342,  0.0763,  0.0263,  0.0132, -0.0547,  0.0041,\n",
      "          0.0068, -0.0614,  0.0043, -0.0043, -0.0659,  0.0143,  0.0898,  0.0536,\n",
      "          0.0771, -0.0008,  0.0305,  0.0074, -0.0773, -0.0518,  0.0816,  0.0267,\n",
      "          0.0784,  0.0748, -0.0527,  0.0752, -0.0840, -0.0260,  0.0587,  0.0859,\n",
      "          0.0265,  0.0353, -0.0790,  0.0690, -0.0334,  0.0005,  0.0017, -0.0120,\n",
      "          0.0279,  0.0548,  0.0891,  0.0358,  0.0619, -0.0695,  0.0274, -0.0826,\n",
      "          0.0791,  0.0567,  0.0533,  0.0087, -0.0347]], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0789,  0.0025,  0.0320, -0.0146, -0.0162], device='cuda:0',\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97843dd7617747c1a1d9b614d6fb38f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7bf5eb05e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shate/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/shate/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1430, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shate/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mlp_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_dl_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg_beat_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_model_provider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mml_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mml_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m mlp_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval/auroc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/praca_magisterska/medical_signal_representation/notebooks/models/funcs.py:53\u001b[0m, in \u001b[0;36mrun_dl_experiment\u001b[0;34m(rep_type, model_provider, train_transform, inference_transform, task)\u001b[0m\n\u001b[1;32m     51\u001b[0m dl_trainer \u001b[38;5;241m=\u001b[39m TrainerClass(trainer, model, dm)\n\u001b[1;32m     52\u001b[0m dl_trainer\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m---> 53\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mdl_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/Desktop/praca_magisterska/medical_signal_representation/msr/training/trainers.py:100\u001b[0m, in \u001b[0;36mBaseTrainer.evaluate\u001b[0;34m(self, plotter, logger)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, plotter: BasePlotter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, logger: MLWandbLogger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     98\u001b[0m     all_y_values \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# \"train\": {\"preds\": self.predict(self.datamodule.train.data), \"target\": self.datamodule.train.targets},\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39mval\u001b[38;5;241m.\u001b[39mtargets},\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39mtest_data), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39mtest\u001b[38;5;241m.\u001b[39mtargets},\n\u001b[1;32m    102\u001b[0m     }\n\u001b[1;32m    104\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {split: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metrics(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39my_values) \u001b[38;5;28;01mfor\u001b[39;00m split, y_values \u001b[38;5;129;01min\u001b[39;00m all_y_values\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    105\u001b[0m     evaluation_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mjson_normalize(metrics, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# flattened dict\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     }\n",
      "File \u001b[0;32m~/Desktop/praca_magisterska/medical_signal_representation/msr/training/trainers.py:135\u001b[0m, in \u001b[0;36mDLTrainer.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/praca_magisterska/medical_signal_representation/msr/models/modules.py:22\u001b[0m, in \u001b[0;36mBaseModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/praca_magisterska/medical_signal_representation/msr/models/architectures/networks/base.py:17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/praca_magisterska/medical_signal_representation/msr/models/architectures/blocks/feature_extractor.py:11\u001b[0m, in \u001b[0;36mFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/praca_magisterska/medical_signal_representation/msr/models/architectures/blocks/mlp.py:79\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/praca_magisterska/medical_signal_representation/msr/models/architectures/blocks/mlp.py:44\u001b[0m, in \u001b[0;36mFeedForwardBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/msr-xbuxOujG-py3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "mlp_results = run_dl_experiment(\"agg_beat_features\", mlp_model_provider, ml_transform, ml_transform)\n",
    "mlp_results['metrics']['val/auroc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6840fa50-67d7-45fd-bc6d-e56333e8955c",
   "metadata": {},
   "source": [
    "# **CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a1da5a9-8918-4c1f-abc3-08593eabec29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | net       | CNNClassifier | 565   \n",
      "1 | criterion | NLLLoss       | 0     \n",
      "--------------------------------------------\n",
      "565       Trainable params\n",
      "0         Non-trainable params\n",
      "565       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa24b33101174f6d9094daea139a036e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6715338826179504"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_transform = Permute(dims=(1, 0))\n",
    "cnn_results = run_dl_experiment(\"agg_beat_waveforms\", cnn_model_provider, cnn_transform, cnn_transform)\n",
    "cnn_results['metrics']['val/auroc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5cd85a-9988-4993-9848-98df5e64995f",
   "metadata": {},
   "source": [
    "# **LGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6cfc955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702d297c3f244cdebe29a709926bab5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole_signal_waveforms 0.82\n",
      "whole_signal_features 0.93\n",
      "agg_beat_waveforms 0.91\n",
      "agg_beat_features 0.91\n"
     ]
    }
   ],
   "source": [
    "lgbm_results = run_all_ml_experiments(lgbm_model_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabce0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1e1d696ca6128c8d724f5fa852bc05b16e6f960aad2a92ccc7563619dcbe50f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
