{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7a7bab-737f-4826-afd2-f76003ed6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wfdb\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "import wget\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import shutil\n",
    "from itertools import groupby\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e77004-580d-4d5d-953f-d45479f8ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good_measurement(record, measurement):\n",
    "    header = requests.get(f\"https://physionet.org/files/{MIMIC_DB_NAME}/1.0/{record}/{measurement}.hea\").text\n",
    "    return {\n",
    "        \"path\": f\"{record}/{measurement}\",\n",
    "        **{signal: signal in header for signal in [\"ABP\", \"PLETH\", \"II\", \"V\"]}\n",
    "    }\n",
    "\n",
    "def find_good_measurements(all_records, start, n_records=200):\n",
    "    end = start + n_records\n",
    "    current_records = all_records[start:end]\n",
    "    good_measurements = []\n",
    "    for record in tqdm(current_records, desc=\"Records\"):\n",
    "        record_measurements = requests.get(f\"https://physionet.org/files/{MIMIC_DB_NAME}/1.0/{record}/RECORDS\").text.split(\"\\n\")\n",
    "        record_measurements = [measurement for measurement in record_measurements if len(measurement) > 0]\n",
    "        results = Parallel(n_jobs=-1)(delayed(is_good_measurement)(record, measurement) for measurement in tqdm(record_measurements, desc=\"Record measurements\"))\n",
    "        results = [result for result in results if result is not None]\n",
    "        good_measurements.extend(results)\n",
    "        time.sleep(0.1)\n",
    "    df = pd.DataFrame(good_measurements)\n",
    "    df.to_csv(f\"mimic_recordings/{start}-{end}.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "def download_measurement(measurement_path):\n",
    "    try:\n",
    "        directory, subject, recording = measurement_path.split(\"/\")\n",
    "        path = Path(f\"mimic_db/{directory}/{subject}\")\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        url_dat = f'https://physionet.org/files/mimic3wdb-matched/1.0/{directory}/{subject}/{recording}.dat'\n",
    "        url_hea = f'https://physionet.org/files/mimic3wdb-matched/1.0/{directory}/{subject}/{recording}.hea'\n",
    "        wget.download(url_dat, out=str(path))\n",
    "        wget.download(url_hea, out=str(path))\n",
    "        with open(\"downloaded_measurements.txt\", \"a\") as file:\n",
    "            file.write(f\"{measurement_path}\\n\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        print(f\"   {measurement_path} failed\")\n",
    "        return False\n",
    "        \n",
    "def download_measurements(measurements, n_jobs=-1):  \n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(download_measurement)(path) for path in tqdm(measurements, desc=\"Downloading measurements\"))\n",
    "    return results\n",
    "\n",
    "def validate(path, desired_signals=['ABP', 'II', 'PLETH']):\n",
    "    try:\n",
    "        data, info = wfdb.rdsamp(f'mimic_db/{path}')\n",
    "        df = pd.DataFrame(data, columns=info['sig_name'])\n",
    "        if any(sig_name not in info['sig_name'] for sig_name in desired_signals):\n",
    "            return False\n",
    "        is_nans_good = all([nans / len(df) < 0.95 for nans in [np.isnan(df[sig_name].values).sum() for sig_name in desired_signals]])\n",
    "        return is_nans_good\n",
    "    except Exception:\n",
    "        print(f\"{path} failed\")\n",
    "        return False\n",
    "    \n",
    "def remove_measurements(measurements):\n",
    "    for path in tqdm(measurements, desc=\"Removing invalid measurements\"):\n",
    "        try:\n",
    "            dir_path = f\"mimic_db/{path}\"\n",
    "            os.remove(f\"{dir_path}.hea\")\n",
    "            os.remove(f\"{dir_path}.dat\")\n",
    "            shutil.rmtree(dir_path)\n",
    "            with open(\"removed_measurements.txt\", \"a\") as file:\n",
    "                file.write(f\"{dir_path}\\n\")\n",
    "        except Exception as e:\n",
    "            print(dir_path, f\" failed ({e})\")\n",
    "\n",
    "def find_repeats(arr, min_n_repeats):\n",
    "    \"\"\"Finds contiguous True regions of the boolean array \"condition\". Returns\n",
    "    a 2D array where the first column is the start index of the region and the\n",
    "    second column is the end index.\"\"\"\n",
    "    condition = np.concatenate((arr[:-1] == arr[1:], np.array([False])))\n",
    "    d = np.diff(condition)\n",
    "    idx, = d.nonzero() \n",
    "    idx += 1\n",
    "    if condition[0]:\n",
    "        idx = np.r_[0, idx]\n",
    "    if condition[-1]:\n",
    "        idx = np.r_[idx, condition.size] # Edit\n",
    "    idx.shape = (-1,2)\n",
    "    return [segment.tolist() for segment in idx if (segment[1] - segment[0]) >= min_n_repeats - 1]\n",
    "\n",
    "def validate_signal(data, low=None, high=None, n_same=10, min_good_samps=60):\n",
    "    if len(data) < min_good_samps:\n",
    "        return np.zeros_like(data) == 1\n",
    "    \n",
    "    no_nans = ~np.isnan(data)\n",
    "    if low is None or high is None:\n",
    "        good_range = np.zeros_like(data) == 0\n",
    "    else:\n",
    "        good_range = (data > low) & (data < high)\n",
    "    \n",
    "    different_vals = np.zeros_like(data) == 0\n",
    "    repeats_bounds = find_repeats(data, n_same)\n",
    "    for start_idx, end_idx in repeats_bounds:\n",
    "        different_vals[start_idx : end_idx+1] = False\n",
    "        \n",
    "    return no_nans & good_range & different_vals\n",
    "\n",
    "def validate_recording(abp, ppg, ecg, fs=125, n_same=10, min_good_secs=30):\n",
    "    min_good_samps = min_good_secs * fs\n",
    "    abp_is_good = validate_signal(abp, low=0, high=300, n_same=n_same)\n",
    "    ppg_is_good = validate_signal(ppg, low=None, high=None, n_same=n_same)\n",
    "    ecg_is_good = validate_signal(ecg, low=None, high=None, n_same=n_same)\n",
    "    is_good = abp_is_good & ppg_is_good & ecg_is_good\n",
    "    return is_good\n",
    "\n",
    "def find_good_segments(abp, ppg, ecg, fs=125, n_same=10, min_good_secs=30):\n",
    "    is_good = validate_recording(abp, ppg, ecg, fs, n_same, min_good_secs)\n",
    "    segments = [(group[0], group[-1]) for group in (list(group) for key, group in groupby(range(len(is_good)), key=is_good.__getitem__) if key)]\n",
    "    good_segments = [segment for segment in segments if (segment[1] - segment[0]) / fs > min_good_secs]\n",
    "    return good_segments\n",
    "\n",
    "def find_valid_segments(path, n_same=10, min_good_secs=120, return_segments=False):\n",
    "    try:\n",
    "        data, info = wfdb.rdsamp(f'mimic_db/{path}')\n",
    "        fs = info['fs']\n",
    "        sig_names = info['sig_name']\n",
    "\n",
    "        abp = data[:, sig_names.index(\"ABP\")] \n",
    "        ppg = data[:, sig_names.index(\"PLETH\")] \n",
    "        ecg = data[:, sig_names.index(\"II\")] \n",
    "\n",
    "        valid_segments = find_good_segments(abp, ppg, ecg, fs=FS, n_same=n_same, min_good_secs=min_good_secs)\n",
    "        dir_path = Path(\"mimic_db\") / path\n",
    "        for start, end in valid_segments:\n",
    "            segment_path = dir_path / f\"{start}-{end}\"\n",
    "            with open(\"valid_segments.txt\", \"a\") as file:\n",
    "                file.write(f\"{str(segment_path)}\\n\")\n",
    "        with open(\"segmented_measurements.txt\", \"a\") as file:\n",
    "            file.write(f\"{path}\\n\")\n",
    "        with open(\"logs.txt\", \"a\") as file:\n",
    "            file.write(f\"{path} OK ({len(valid_segments)} segments)\\n\")\n",
    "        if return_segments:\n",
    "            return valid_segments\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        with open(\"logs.txt\", \"a\") as file:\n",
    "            file.write(f\"{path} failed ({e})\\n\")\n",
    "        print(f\"{path} failed ({e})\")\n",
    "        return False\n",
    "    \n",
    "def load_segment_to_df(path, start, end):\n",
    "    data, info = wfdb.rdsamp(path)\n",
    "    fs = info['fs']\n",
    "    sig_names = info['sig_name']\n",
    "    abp = data[start:end, sig_names.index(\"ABP\")] \n",
    "    ppg = data[start:end, sig_names.index(\"PLETH\")] \n",
    "    ecg = data[start:end, sig_names.index(\"II\")] \n",
    "    return pd.DataFrame({\"ABP\": abp, \"PPG\": ppg, \"ECG\": ecg})\n",
    "\n",
    "def get_records_samples_bounds(records_bounds, sample_len_samples, shuffle_segments=True):\n",
    "    records_samples_bounds = {}\n",
    "    for record, record_bounds in records_bounds.items():\n",
    "        record_segments_bounds = []\n",
    "        for start, end in record_bounds:\n",
    "            segment_start = start\n",
    "            segment_end = start + sample_len_samples\n",
    "            while segment_end < end:\n",
    "                record_segments_bounds.append((segment_start, segment_end))\n",
    "                segment_start, segment_end = segment_end, segment_end + SAMPLE_LEN_SAMPLES\n",
    "        records_samples_bounds[record] = record_segments_bounds\n",
    "    if shuffle_segments:\n",
    "        for record, segments_bounds in records_samples_bounds.items():\n",
    "            random.shuffle(segments_bounds)\n",
    "    return records_samples_bounds\n",
    "\n",
    "def cut_segments_for_subjects(subjects, subjects_records, records_bounds, max_samples_per_subject, sample_len_samples):\n",
    "    samples_bounds = get_records_samples_bounds(records_bounds, sample_len_samples)\n",
    "    subjects_data = {}\n",
    "    \n",
    "    for subject in tqdm(subjects, desc=\"Cutting subjects recordings into segments\"):\n",
    "        n_samples = 0\n",
    "        record_idx = 0\n",
    "        subject_records = subjects_records[subject]\n",
    "        subject_data = []\n",
    "        while n_samples < max_samples_per_subject:\n",
    "            current_record = subject_records[record_idx]\n",
    "            record_path = f\"{subject}/{current_record}\"\n",
    "            possible_segments_bounds = samples_bounds[record_path]\n",
    "            try:\n",
    "                sample_start_idx, sample_end_idx = possible_segments_bounds.pop()\n",
    "            except:\n",
    "                break # no more samples\n",
    "            sample_path = f\"{record_path}/{sample_start_idx}-{sample_end_idx}\"\n",
    "            if sample_path not in subject_data:\n",
    "                subject_data.append({'path': record_path, 'start': sample_start_idx, 'end': sample_end_idx})\n",
    "            n_samples += 1\n",
    "            record_idx += 1\n",
    "            if record_idx > len(subject_records) - 1:\n",
    "                record_idx = 0\n",
    "        subjects_data[subject] = subject_data\n",
    "    return subjects_data\n",
    "\n",
    "def create_dataset(valid_segments, max_samples_per_subject, sample_len_samples, seed=42):\n",
    "    subjects = np.unique([\"/\".join(m['path'].split(\"/\")[1:3]) for m in valid_segments])\n",
    "    print(f\" Found {len(subjects)} unique subjects\")\n",
    "    records = np.unique([\"/\".join(segment['path'].split('/')[1:]) for segment in valid_segments])\n",
    "    print(f\" Found {len(records)} valid records with total of {len(valid_segments)} valid segments\")\n",
    "    records_bounds = {\"/\".join(record.split(\"/\")[1:]): [(segment['start'], segment['end']) for segment in segments] for record, segments in groupby(valid_segments, key = itemgetter('path'))}\n",
    "    subjects_segments = {subject: [segment for segment in valid_segments if subject in segment['path']] for subject in subjects}\n",
    "    subjects_records = {subject: np.unique([segment['path'].split(\"/\")[3] for segment in segments]).tolist() for subject, segments in subjects_segments.items()}\n",
    "    \n",
    "    print(f\" Segments cutting in progress..\")\n",
    "    random.seed(seed)\n",
    "    subjects_datataset_info = cut_segments_for_subjects(subjects, subjects_records, records_bounds, MAX_SAMPLES_PER_SUBJECT, SAMPLE_LEN_SAMPLES)\n",
    "    \n",
    "    with open(\"samples_segmentation_info.txt\", \"a\") as file:\n",
    "        file.write(\"sample,segment\\n\")\n",
    "        \n",
    "    def save_sample_to_csv(i, subject, sample):\n",
    "        try:\n",
    "            path, start, end = sample['path'], sample['start'], sample['end']\n",
    "            subject = path.split(\"/\")[1]\n",
    "            path = f'mimic_db/{path}'\n",
    "            df = load_segment_to_df(path, start, end)\n",
    "            df.to_csv(f\"mimic_csv/{subject}_{i}.csv\", index=False)\n",
    "            with open(\"samples_segmentation_info.txt\", \"a\") as file:\n",
    "                file.write(f\"{subject}_{i},{path}/{start}-{end}\\n\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"{i}, {sample} failes\")\n",
    "            return False\n",
    "        \n",
    "    for subject, samples in tqdm(subjects_datataset_info.items(), desc=f\"Saving subjects segments into csv files ({SAMPLE_LEN_SEC}s samples)\"):\n",
    "        saved_samples = Parallel(n_jobs=4)(delayed(save_sample_to_csv)(i, subject, sample) for i, sample in enumerate(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a535d-3d7f-4823-9a1c-fb7d2e4784ad",
   "metadata": {},
   "source": [
    "# **Defining global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3db90f9-ddf6-4d04-975d-122bb69b941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_DB_NAME = \"mimic3wdb-matched\"         # specify type of mimic databes (mimic3wdb or mimic3wdb-matched)\n",
    "N_RECORDS = 250                             # number of records for a single `find_good_measurements` run\n",
    "FS = 125                                    # sampling frequency of mimic3wdb signals\n",
    "MAX_SAMPLES_PER_SUBJECT = 30                # maximum number of samples per subject in created dataset\n",
    "SAMPLE_LEN_SEC = 120                        # desired length of sample in seconds\n",
    "SAMPLE_LEN_SAMPLES = SAMPLE_LEN_SEC * FS    # desired length of sample in samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35487454-a380-4d34-903f-339c16f0a5c7",
   "metadata": {},
   "source": [
    "# **Finding good measurements using `*.hea` files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7fa87-1a2d-4a45-9aa5-b1312f3c57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records = requests.get(f\"https://physionet.org/files/{MIMIC_DB_NAME}/1.0/RECORDS\").text.split(\"/\\n\")\n",
    "all_records = [record for record in all_records if len(record) > 0]\n",
    "_ = find_good_measurements(all_records, start=8500, n_records=N_RECORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203173f0-1a92-49f9-974d-bebb52bf7f20",
   "metadata": {},
   "source": [
    "# **Downloading measurements validated by `*.hea` files (measurements with `ABP`, `PLETH` and `II` signals)**\n",
    "\n",
    "(`II` was in some cases mistaken with `III`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32e3f2-8b98-47b1-896e-1483c06f36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_df = pd.DataFrame()\n",
    "\n",
    "for start in np.arange(0, 10000, N_RECORDS):\n",
    "    end = start + N_RECORDS\n",
    "    try:\n",
    "        df = pd.read_csv(f\"mimic_recordings/{start}-{end}.csv\")\n",
    "        measurements_df = pd.concat([measurements_df, df])\n",
    "    except:\n",
    "        print(f'No file for {start}-{end}')\n",
    "        \n",
    "good_measurements = measurements_df.query(\"ABP == True and PLETH == True and II == True\")['path'].values\n",
    "\n",
    "with open('downloaded_measurements.txt') as f:\n",
    "    downloaded_measurements = f.readlines()\n",
    "    downloaded_measurements = [path.replace(\"\\n\", \"\") for path in downloaded_measurements]\n",
    "\n",
    "measurements_to_download = [path for path in good_measurements if path not in downloaded_measurements]\n",
    "\n",
    "print(f\"Downloaded {len(downloaded_measurements)} measurements, attempting {len(measurements_to_download)} measurements download\")\n",
    "\n",
    "download_measurements(measurements_to_download, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a9b3d-7403-457f-a93a-b80aec1f2c20",
   "metadata": {},
   "source": [
    "# **Basic validation**\n",
    "\n",
    "Check if:\n",
    "* `ABP`, `PLETH` and `II` are in the measurement\n",
    "* all signals have less than 95% of nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a843b-5f1e-4312-8331-4e280cf0c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('downloaded_measurements.txt') as f:\n",
    "    downloaded_measurements = f.readlines()\n",
    "    downloaded_measurements = np.array([path.replace(\"\\n\", \"\") for path in downloaded_measurements])\n",
    "    \n",
    "with open('valid_measurements.txt') as f:\n",
    "    valid_measurements = f.readlines()\n",
    "    valid_measurements = np.array([path.replace(\"\\n\", \"\") for path in valid_measurements])\n",
    "    \n",
    "measurements_to_validate = np.array([path for path in downloaded_measurements if path not in valid_measurements])\n",
    "print(f\" Validated {len(valid_measurements)} measurements\\n Attempting validation of {len(measurements_to_validate)} measurements..\")\n",
    "\n",
    "valid_records = Parallel(n_jobs=-1)(delayed(validate)(path) for path in tqdm(measurements_to_validate, desc=\"Validating measurements\"))\n",
    "\n",
    "valid_measurements = np.concatenate((valid_measurements, measurements_to_validate[valid_records]))\n",
    "\n",
    "textfile = open(\"valid_measurements.txt\", \"w\")\n",
    "for measurement_path in valid_measurements:\n",
    "    textfile.write(measurement_path + \"\\n\")\n",
    "textfile.close()\n",
    "\n",
    "unique_subjects = np.unique([m.split(\"/\")[1] for m in valid_measurements])\n",
    "len(unique_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92f6bb-b973-44f3-a3d6-3375e334d9e3",
   "metadata": {},
   "source": [
    "# **Segmenting measurements**\n",
    "\n",
    "Find segments in measurement which meet the conditions:\n",
    "* Atleast 120 sec of good signal, defined by:\n",
    "    * No nans in any of `ABP`, `PLETH` and `II` signals\n",
    "    * No *const* values in any of `ABP`, `PLETH` and `II` signals for more than 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d6cab-d94e-47ed-bcd2-2ed04e669fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('segmented_measurements.txt') as f:\n",
    "    segmented_measurements = f.readlines()\n",
    "    segmented_measurements = [path.replace(\"\\n\", \"\") for path in segmented_measurements]\n",
    "    \n",
    "measurements_to_segment = [path for path in valid_measurements if path not in segmented_measurements]\n",
    "print(f\" Segmented {len(segmented_measurements)} measurements\\n Attempting segmentation of {len(measurements_to_segment)} measurements..\")\n",
    "\n",
    "segmented_measurements = Parallel(n_jobs=-1)(delayed(find_valid_segments)(path) for path in tqdm(measurements_to_segment, desc=\"Segmenting measurements\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855caf7-290c-4ec0-b25c-171aadcc29ac",
   "metadata": {},
   "source": [
    "# **Removing invalid measurements**\n",
    "\n",
    "Remove measurements which didnt meet the conditions defined in segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685cfaa-d807-432a-8eba-8aeac9a3cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_measurements = pd.read_csv(\"downloaded_measurements.txt\", names=['path'])['path'].values\n",
    "removed_measurements = pd.read_csv(\"removed_measurements.txt\", names=['path'])['path'].values\n",
    "removed_measurements = [\"/\".join(path.split(\"/\")[1:]) for path in removed_measurements]\n",
    "\n",
    "valid_segments = pd.read_csv(\"valid_segments.txt\", names=['path'])\n",
    "valid_measurements = np.unique([\"/\".join(path.split(\"/\")[1:-1]) for path in valid_segments['path'].values])\n",
    "measurements_to_remove = [measurement for measurement in downloaded_measurements if measurement not in valid_measurements and measurement not in removed_measurements]\n",
    "\n",
    "# remove_measurements(measurements_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807b901-0c9f-44bd-b2a0-cb229f26772f",
   "metadata": {},
   "source": [
    "# **Creating dataset**\n",
    "\n",
    "Save to csv valid segments (maximum of 30 per subject with 120s long signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c94145-e4a8-45b6-966d-85da7c64e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('valid_segments.txt') as f:\n",
    "    valid_segments = f.readlines()\n",
    "    valid_segments = np.array([path.replace(\"\\n\", \"\") for path in valid_segments])\n",
    "\n",
    "valid_segments = [{'path': \"/\".join(segment.split(\"/\")[:-1]), 'start': int(segment.split(\"/\")[-1].split(\"-\")[0]), 'end': int(segment.split(\"/\")[-1].split(\"-\")[1])} for segment in valid_segments]\n",
    "\n",
    "create_dataset(valid_segments, MAX_SAMPLES_PER_SUBJECT, SAMPLE_LEN_SAMPLES, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c213b1-7c6a-4a14-959d-d9c5b06ad095",
   "metadata": {},
   "source": [
    "# **Filling info about dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a1e3d-15c3-401f-b479-f6d0de1b60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_info = pd.read_csv(\"samples_segmentation_info.txt\")\n",
    "fs = np.array([FS]*len(seg_info))\n",
    "samples_path = np.array([f\"{sample}.csv\" for sample in seg_info['sample']])\n",
    "subjects = np.array([sample.split(\"_\")[0] for sample in seg_info['sample']])\n",
    "subjects_sample_number = np.array([int(sample.split(\"_\")[1]) for sample in seg_info['sample']])\n",
    "recording_path = np.array([\"/\".join(segment_path.split(\"/\")[1:-1]) for segment_path in seg_info['segment']])\n",
    "samples_start_idxs = np.array([int(sample.split(\"/\")[-1].split(\"-\")[0]) for sample in seg_info['segment']])\n",
    "samples_end_idxs = np.array([int(sample.split(\"/\")[-1].split(\"-\")[1]) for sample in seg_info['segment']])\n",
    "\n",
    "dataset_info = pd.DataFrame({\n",
    "    \"sample_path\": samples_path,\n",
    "    \"subject\": subjects,\n",
    "    \"sample_num\": subjects_sample_number,\n",
    "    \"recording\": recording_path,\n",
    "    \"sample_start_idx\": samples_start_idxs,\n",
    "    \"sample_end_idx\": samples_end_idxs,\n",
    "    \"fs\": fs\n",
    "}).sort_values([\"subject\", \"sample_num\"]).reset_index(drop=True)\n",
    "dataset_info.to_csv(\"dataset_info.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
